Advanced Data Pipeline & Dashboard Generator (PySpark + Streamlit)

This project demonstrates a professional, two-part data engineering workflow for processing massive, raw datasets and visualizing them in a high-performance, interactive dashboard.

It evolves from a simple "upload-and-visualize" app into a robust pipeline that can handle data files (like Parquet or CSV) that are too large to fit into memory (GB-scale), solving common memory crashes and performance bottlenecks.

The Professional Workflow

This project is split into two distinct components, mimicking a real-world data team structure (Data Engineers & Data Analysts).

1. The "Processor" (app1/) - Data Engineering

Technology: PySpark

Purpose: Handles the heavy lifting. This script reads a massive raw data file (e.g., a 2GB Parquet file) that would crash a normal Pandas-based application.

Job:

Loads the large file efficiently from disk (not into RAM).

Cleans the data at scale (e.g., converting text-based numbers, handling errors).

Samples the entire dataset to extract a representative subset (e.g., 100,000 random rows).

Saves this small, clean sample as a new CSV file (cleaned_sample.csv).

2. The "Visualizer" (app/) - Data Analysis

Technology: Streamlit, Pandas, Plotly

Purpose: Provides a fast, interactive, and user-friendly web dashboard.

Job:

Loads the small, pre-processed cleaned_sample.csv (which is fast and light on memory).

Provides an interactive UI with dropdowns and toggles.

Generates a full suite of visualizations (KPIs, Trends, Pies, Heatmaps, Scatter plots) based on the clean sample.

Key Improvements Over a Basic Dashboard

This project is a significant step up from a simple single-file Streamlit app.

Scalability: The PySpark processor can handle datasets of virtually any size (GBs or TBs), something a Pandas-only app (pd.read_csv) cannot.

Performance: The Streamlit dashboard is extremely fast and responsive. Users only load the lightweight sample, so changing dropdowns doesn't cause crashes or long wait times.

Robustness: The dashboard charts are crash-proof. Pie and Bar charts automatically group small categories into an "Other" slice (Top 10/15 logic), preventing crashes when selecting columns with thousands of unique values.

Flexibility: The Streamlit app (app/) is still a powerful standalone tool! You can skip the Spark processor and use it directly to upload and analyze any small-to-medium-sized CSV, JSON, or Excel file, just like the original app.

How to Use

Part 1: The Big Data Workflow (Recommended)

Use this workflow for GB-scale files.

Prerequisites:

Install Java JDK 11 (required for PySpark).

Create a virtual environment: python -m venv venv

Activate it: source venv/bin/activate

Install Dependencies:

# Install Spark processor requirements
pip install -r app1/requirements_processing.txt

# Install dashboard app requirements
pip install -r app/requirements_app.txt


Place Your Data:

Copy your large raw file (e.g., yellow_tripdata_2017-01.parquet) into the data/raw/ directory.

Configure & Run the Processor:

Open app1/process_data.py.

Update RAW_FILE_PATH (Line 11) to match your file's name.

(If not .parquet) Update the spark.read function (Line 41) to spark.read.csv(...).

Run the script:

python app1/process_data.py


Wait for it to complete. A new file, cleaned_sample.csv, will be created in data/processed/.

Run the Dashboard:

streamlit run app/app.py


Visualize:

In the browser, upload the new data/processed/cleaned_sample.csv file.

Enjoy a fast, crash-free dashboard!

Part 2: The Small File Workflow

Use this for any regular CSV, JSON, or Excel file that fits in memory.

Skip Part 1.

Run the Dashboard:

streamlit run app/app.py


Visualize:

Upload your small file directly. The app will clean and analyze it on the fly.

Technology Stack

Data Processing: PySpark (for scalable, out-of-memory processing)

Dashboard UI: Streamlit

Data Manipulation: Pandas (for in-memory manipulation within the dashboard)

Visualization: Plotly Express