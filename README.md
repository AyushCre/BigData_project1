ğŸš€ Advanced Data Pipeline & Dashboard Generator | PySpark + Streamlit + Plotly  

Developed a scalable, endâ€‘toâ€‘end big data pipeline that processes GBâ€‘scale datasets using PySpark and visualizes them in a highâ€‘performance, interactive Streamlit dashboard. This solution overcomes memory overflow issues by separating heavy data processing from fast, crashâ€‘proof visualization.

âš™ï¸ 2â€‘Part Workflow  
â€¢ **Processor (app1/):** PySpark cleans and processes large raw data files (e.g., 2GB .parquet), extracts a representative 100Kâ€‘row sample, and saves it as `cleaned_sample.csv`.  
â€¢ **Visualizer (app/):** Streamlit, Plotly, and Pandas power the dashboard to explore aggregated data interactively via KPIs, trends, and heatmaps.

ğŸ“Š Key Highlights  
â€¢ **Scalability:** Distributed PySpark handles multiâ€‘GB/TB datasets.  
â€¢ **Performance:** Streamlit dashboards load instantly from small samples.  
â€¢ **Robustness:** Topâ€‘10/15 grouping logic avoids memory overflow.  
â€¢ **Flexibility:** Can also load small CSV/Excel/JSON data directly without Spark.

ğŸ’» **Setup & Commands**

ğŸ‘‰ [Create and activate virtual environment](python -m venv venv && source venv/bin/activate)  

ğŸ‘‰ [Install dependencies](pip install -r app1/requirements_processing.txt && pip install -r app/requirements_app.txt)  

ğŸ‘‰ [Place your data file](data/raw/)  

ğŸ‘‰ [Edit configuration paths](app1/process_data.py â€” Line 11: RAW_FILE_PATH, Line 41: spark.read)  

ğŸ‘‰ [Run the PySpark processor](python app1/process_data.py)  

ğŸ‘‰ [Launch the Streamlit dashboard](streamlit run app/app.py)  

ğŸ‘‰ [Upload the processed file](data/processed/cleaned_sample.csv)

ğŸ§  **Tech Stack:** PySpark | Streamlit | Plotly | Pandas | Python  

For a detailed explanation and demo video, please visit my GitHub repository.  
To explore the entire codebase and workflow, check it out on my GitHub profile.
